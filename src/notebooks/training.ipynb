{"cells":[{"cell_type":"markdown","source":"# Final Project - Training with optimization.","metadata":{"tags":[],"cell_id":"87c2358463c94d95bd9604aadc6db1fe","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"4e0afa0847594ccfa5407b9ee8cad7f3","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nimport numpy as np\nimport random\nimport pandas as pd\n\nseed = 42\nos.environ['PYTHONHASHSEED'] = str(seed)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\nrandom.seed(seed)\ntf.random.set_seed(seed)\nnp.random.seed(seed)","metadata":{"tags":[],"cell_id":"6e01a92be1ff4e22af1d32a0aab9baaf","source_hash":"6b925aea","execution_start":1686494802670,"execution_millis":5165,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2023-06-11 14:46:42.684839: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-06-11 14:46:42.839894: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-06-11 14:46:42.839938: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-06-11 14:46:42.882882: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-06-11 14:46:44.644646: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-06-11 14:46:44.644729: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-06-11 14:46:44.644739: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"PREPROCESSING_ARGS = {\n    'downsampling_rate': 16000,\n    'frame_length_in_s': 0.016,\n    'frame_step_in_s': 0.012,\n    'num_mel_bins': 40,\n    'lower_frequency': 20,\n    'upper_frequency': 8000,\n    'num_coefficients': 30\n}\nTRAINING_ARGS = {\n    'batch_size': 20,\n    'initial_learning_rate': 0.01,\n    'end_learning_rate': 0,\n    'epochs': 20\n}\n\nfinal_sparsity = 0.7 #[0.7; 0.95]\nalpha = 0.4 #[0.25; 0.5; 0.75]","metadata":{"tags":[],"cell_id":"6d40ffc1cd6347df9b3fe3945c9f6807","source_hash":"9bcae67a","execution_start":1686494807877,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Preprocessing","metadata":{"tags":[],"cell_id":"cc3fc0e9714d4d4d90a42acefb2e36be","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"code","source":"from loader import *\ndata_loader = VISCDataLoader(seconds = 2, channels = 1, feature_type='MFCCS', **PREPROCESSING_ARGS)\n","metadata":{"tags":[],"cell_id":"3db930d0905d4243953d00f1c837a715","source_hash":"30d49b99","execution_start":1686494807878,"execution_millis":44,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"batch_size = TRAINING_ARGS['batch_size']\nepochs = TRAINING_ARGS['epochs']\n\ntrain_ds = data_loader.get_split('train')\ntrain_ds = train_ds.cache()\ntrain_ds = train_ds.batch(batch_size)\nprint(train_ds)\nval_ds = data_loader.get_split('val').batch(batch_size)\ntest_ds = data_loader.get_split('test').batch(batch_size)","metadata":{"tags":[],"cell_id":"e42d18ed68324e31bd15b9081034e8d2","source_hash":"3d9cc67","execution_start":1686494807921,"execution_millis":5529,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2023-06-11 14:46:47.902390: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2023-06-11 14:46:47.902424: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2023-06-11 14:46:47.902445: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-05dea53b-e624-4a45-a48c-4f8a58ba823b): /proc/driver/nvidia/version does not exist\n2023-06-11 14:46:47.902728: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-06-11 14:46:48.610963: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n2023-06-11 14:46:48.615137: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 AVX512F FMA\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2023-06-11 14:46:49.157103: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2023-06-11 14:46:49.159080: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2023-06-11 14:46:49.159342: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n(166, 30)\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2023-06-11 14:46:50.220434: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2023-06-11 14:46:50.222446: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2023-06-11 14:46:50.222705: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n<BatchDataset element_spec=(TensorSpec(shape=(None, 32, 32, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2023-06-11 14:46:50.754373: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2023-06-11 14:46:50.756324: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2023-06-11 14:46:50.756569: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n(166, 30)\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2023-06-11 14:46:51.700280: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2023-06-11 14:46:51.702319: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2023-06-11 14:46:51.702577: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2023-06-11 14:46:52.216614: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2023-06-11 14:46:52.218631: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2023-06-11 14:46:52.218893: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n(166, 30)\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2023-06-11 14:46:53.161901: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2023-06-11 14:46:53.163791: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2023-06-11 14:46:53.164043: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"for example_batch, example_labels in train_ds.take(1):\n  print('Batch Shape:', example_batch.shape)\n  print('Data Shape:', example_batch.shape[1:])\n  print('Labels:', example_labels)","metadata":{"tags":[],"cell_id":"78b894bad958402da9170d10344ab58c","source_hash":"9dead3aa","execution_start":1686494813456,"execution_millis":831,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Batch Shape: (20, 32, 32, 1)\nData Shape: (32, 32, 1)\nLabels: tf.Tensor([4 7 1 3 0 0 7 7 5 7 6 0 0 7 0 2 3 0 6 2], shape=(20,), dtype=int32)\n2023-06-11 14:46:54.191309: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## DS-CNN","metadata":{"tags":[],"cell_id":"149424dc314a41e9bdafe3367fb61d00","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"markdown","source":"Conv2D(filters=(128*alpha), kernel_size=[3, 3], stride=[2, 2],use_bias=False, padding=’valid’)\nBatchNormalization()\nReLU()\nDepthwiseConv2D(kernel_size=[3, 3], stride=[1, 1],\nuse_bias=False, padding=’same’)\nConv2D(filters=(128*alpha), kernel_size=[1, 1], stride=[1, 1],use_bias=False)\nBatchNormalization()\nReLU()\nDepthwiseConv2D(kernel_size=[3, 3], stride=[1, 1],\nuse_bias=False, padding=’same’)\nConv2D(filters=(128*alpha), kernel_size=[1, 1], stride=[1, 1],use_bias=False)\nBatchNormalization()\nReLU()\nGlobalAveragePooling2D()\nDense(units=len(LABELS)\nSoftmax()","metadata":{"tags":[],"cell_id":"e253d7a0f76f4351b5a41ecf8ddda515","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n    tf.keras.layers.Conv2D(filters=(128*alpha), kernel_size=[3, 3], strides=[2, 2],\n        use_bias=False, padding='valid'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], \n        use_bias=False, padding='same'),\n    tf.keras.layers.Conv2D(filters=(128*alpha), kernel_size=[1, 1], strides=[1, 1],   \n       use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1],\n        use_bias=False, padding='same'),\n    tf.keras.layers.Conv2D(filters=(128*alpha), kernel_size=[1, 1], strides=[1, 1],   \n       use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(units=8),\n    tf.keras.layers.Softmax()\n])","metadata":{"tags":[],"cell_id":"98e850a2e1324e83b0a65271bcaf4f7f","source_hash":"68f52d99","execution_start":1686494814587,"execution_millis":105,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":7},{"cell_type":"code","source":"model.summary()","metadata":{"tags":[],"cell_id":"50c88cb5443e409781c70563fd476545","source_hash":"4e6a3b95","execution_start":1686494814696,"execution_millis":124,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_3 (Conv2D)           (None, 15, 15, 51)        459       \n                                                                 \n batch_normalization_3 (Batc  (None, 15, 15, 51)       204       \n hNormalization)                                                 \n                                                                 \n re_lu_3 (ReLU)              (None, 15, 15, 51)        0         \n                                                                 \n depthwise_conv2d (Depthwise  (None, 15, 15, 51)       459       \n Conv2D)                                                         \n                                                                 \n conv2d_4 (Conv2D)           (None, 15, 15, 51)        2601      \n                                                                 \n batch_normalization_4 (Batc  (None, 15, 15, 51)       204       \n hNormalization)                                                 \n                                                                 \n re_lu_4 (ReLU)              (None, 15, 15, 51)        0         \n                                                                 \n depthwise_conv2d_1 (Depthwi  (None, 15, 15, 51)       459       \n seConv2D)                                                       \n                                                                 \n conv2d_5 (Conv2D)           (None, 15, 15, 51)        2601      \n                                                                 \n batch_normalization_5 (Batc  (None, 15, 15, 51)       204       \n hNormalization)                                                 \n                                                                 \n re_lu_5 (ReLU)              (None, 15, 15, 51)        0         \n                                                                 \n global_average_pooling2d_1   (None, 51)               0         \n (GlobalAveragePooling2D)                                        \n                                                                 \n dense_1 (Dense)             (None, 8)                 416       \n                                                                 \n softmax_1 (Softmax)         (None, 8)                 0         \n                                                                 \n=================================================================\nTotal params: 7,607\nTrainable params: 7,301\nNon-trainable params: 306\n_________________________________________________________________\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Setup Magnitude-based Weights Pruning","metadata":{"tags":[],"cell_id":"b681830a8ea043a688bc9bee07594a18","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"code","source":"import tensorflow_model_optimization as tfmot\n\nprune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n\nbegin_step = int(len(train_ds) * epochs * 0.2)\nend_step = int(len(train_ds) * epochs*0.8)\n\npruning_params = {\n    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n        initial_sparsity=0.20,\n        final_sparsity=final_sparsity,\n        begin_step=begin_step,\n        end_step=end_step\n    )\n}\n\nmodel_for_pruning = prune_low_magnitude(model, **pruning_params)","metadata":{"tags":[],"cell_id":"9fe5589ce9d649369f9e2777767da77e","source_hash":"e33f0cab","execution_start":1686494814819,"execution_millis":1079,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Training","metadata":{"tags":[],"cell_id":"ced63b36087a4705b624abaec42d85b5","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"code","source":"#Run this only if you're not pruning\n#model_for_pruning = model","metadata":{"cell_id":"f0962fac037342e8980c82304d5f34b9","source_hash":"fe3eb18e","execution_start":1686494815899,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":10},{"cell_type":"code","source":"loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\ninitial_learning_rate = TRAINING_ARGS['initial_learning_rate']\nend_learning_rate = TRAINING_ARGS['end_learning_rate']\n\nlinear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=initial_learning_rate,\n    end_learning_rate=end_learning_rate,\n    decay_steps=len(train_ds) * epochs,\n)\noptimizer = tf.optimizers.Adam(learning_rate=linear_decay)\nmetrics = [tf.metrics.SparseCategoricalAccuracy()]\ncallbacks = [tfmot.sparsity.keras.UpdatePruningStep()]\nmodel_for_pruning.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n\nhistory = model_for_pruning.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=callbacks)","metadata":{"cell_id":"28c20bb165374e70b32ea5b747bf8712","source_hash":"41dd4669","execution_start":1686494815948,"execution_millis":600559,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Epoch 1/20\n209/209 [==============================] - 113s 521ms/step - loss: 0.9053 - sparse_categorical_accuracy: 0.7001 - val_loss: 2.1296 - val_sparse_categorical_accuracy: 0.4759\nEpoch 2/20\n209/209 [==============================] - 24s 116ms/step - loss: 0.3476 - sparse_categorical_accuracy: 0.8878 - val_loss: 0.3406 - val_sparse_categorical_accuracy: 0.8666\nEpoch 3/20\n209/209 [==============================] - 24s 116ms/step - loss: 0.2321 - sparse_categorical_accuracy: 0.9284 - val_loss: 0.2012 - val_sparse_categorical_accuracy: 0.9196\nEpoch 4/20\n209/209 [==============================] - 24s 116ms/step - loss: 0.1744 - sparse_categorical_accuracy: 0.9431 - val_loss: 0.1436 - val_sparse_categorical_accuracy: 0.9518\nEpoch 5/20\n209/209 [==============================] - 24s 116ms/step - loss: 0.1183 - sparse_categorical_accuracy: 0.9618 - val_loss: 0.1233 - val_sparse_categorical_accuracy: 0.9598\nEpoch 6/20\n209/209 [==============================] - 24s 116ms/step - loss: 0.1137 - sparse_categorical_accuracy: 0.9640 - val_loss: 0.0690 - val_sparse_categorical_accuracy: 0.9807\nEpoch 7/20\n209/209 [==============================] - 24s 116ms/step - loss: 0.0929 - sparse_categorical_accuracy: 0.9719 - val_loss: 0.1460 - val_sparse_categorical_accuracy: 0.9518\nEpoch 8/20\n209/209 [==============================] - 24s 116ms/step - loss: 0.1195 - sparse_categorical_accuracy: 0.9625 - val_loss: 0.2971 - val_sparse_categorical_accuracy: 0.8907\nEpoch 9/20\n209/209 [==============================] - 24s 116ms/step - loss: 0.0949 - sparse_categorical_accuracy: 0.9724 - val_loss: 0.1890 - val_sparse_categorical_accuracy: 0.9373\nEpoch 10/20\n209/209 [==============================] - 24s 116ms/step - loss: 0.0940 - sparse_categorical_accuracy: 0.9760 - val_loss: 0.1422 - val_sparse_categorical_accuracy: 0.9518\nEpoch 11/20\n209/209 [==============================] - 25s 118ms/step - loss: 0.0747 - sparse_categorical_accuracy: 0.9808 - val_loss: 0.1217 - val_sparse_categorical_accuracy: 0.9550\nEpoch 12/20\n209/209 [==============================] - 25s 118ms/step - loss: 0.0661 - sparse_categorical_accuracy: 0.9853 - val_loss: 0.0995 - val_sparse_categorical_accuracy: 0.9678\nEpoch 13/20\n209/209 [==============================] - 25s 121ms/step - loss: 0.0563 - sparse_categorical_accuracy: 0.9870 - val_loss: 0.0749 - val_sparse_categorical_accuracy: 0.9791\nEpoch 14/20\n209/209 [==============================] - 25s 121ms/step - loss: 0.0454 - sparse_categorical_accuracy: 0.9897 - val_loss: 0.0533 - val_sparse_categorical_accuracy: 0.9871\nEpoch 15/20\n209/209 [==============================] - 26s 124ms/step - loss: 0.0420 - sparse_categorical_accuracy: 0.9901 - val_loss: 0.0498 - val_sparse_categorical_accuracy: 0.9839\nEpoch 16/20\n209/209 [==============================] - 27s 128ms/step - loss: 0.0341 - sparse_categorical_accuracy: 0.9938 - val_loss: 0.0368 - val_sparse_categorical_accuracy: 0.9871\nEpoch 17/20\n209/209 [==============================] - 25s 121ms/step - loss: 0.0303 - sparse_categorical_accuracy: 0.9942 - val_loss: 0.0346 - val_sparse_categorical_accuracy: 0.9887\nEpoch 18/20\n209/209 [==============================] - 25s 121ms/step - loss: 0.0273 - sparse_categorical_accuracy: 0.9954 - val_loss: 0.0319 - val_sparse_categorical_accuracy: 0.9904\nEpoch 19/20\n209/209 [==============================] - 25s 121ms/step - loss: 0.0253 - sparse_categorical_accuracy: 0.9952 - val_loss: 0.0284 - val_sparse_categorical_accuracy: 0.9920\nEpoch 20/20\n209/209 [==============================] - 25s 121ms/step - loss: 0.0238 - sparse_categorical_accuracy: 0.9959 - val_loss: 0.0275 - val_sparse_categorical_accuracy: 0.9936\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Testing","metadata":{"tags":[],"cell_id":"60afa6abc24842dab882e5e5cb52bf95","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"code","source":"test_loss, test_accuracy = model_for_pruning.evaluate(test_ds)","metadata":{"tags":[],"cell_id":"c4a7c015d75c441388f3cc86d264b3b9","source_hash":"ddc77961","execution_start":1686495416509,"execution_millis":27998,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"60/60 [==============================] - 28s 455ms/step - loss: 0.0547 - sparse_categorical_accuracy: 0.9858\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Recap","metadata":{"tags":[],"cell_id":"ff36cc65bed0477db1b49b2b766b4403","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"code","source":"training_loss = history.history['loss'][-1]\ntraining_accuracy = history.history['sparse_categorical_accuracy'][-1]\nval_loss = history.history['val_loss'][-1]\nval_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n\nprint(f'Training Loss: {training_loss:.4f}')\nprint(f'Training Accuracy: {training_accuracy*100.:.2f}%')\nprint()\nprint(f'Validation Loss: {val_loss:.4f}')\nprint(f'Validation Accuracy: {val_accuracy*100.:.2f}%')\nprint()\nprint(f'Test Loss: {test_loss:.4f}')\nprint(f'Test Accuracy: {test_accuracy*100.:.2f}%')","metadata":{"tags":[],"cell_id":"3397172eb4d848b2b3212ebc0eca75b5","source_hash":"d8271375","execution_start":1686495444497,"execution_millis":11,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Training Loss: 0.0238\nTraining Accuracy: 99.59%\n\nValidation Loss: 0.0275\nValidation Accuracy: 99.36%\n\nTest Loss: 0.0547\nTest Accuracy: 98.58%\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import pandas as pd\nfrom time import time\n\ntimestamp = int(time())\n\noutput_dict = {\n    'timestamp': timestamp,\n    'feature_type': data_loader.get_feature_type(),\n    **PREPROCESSING_ARGS,\n    **TRAINING_ARGS,\n    'test_accuracy': test_accuracy,\n}\n\ndf = pd.DataFrame([output_dict])\n\noutput_path='./mfccs_spectrogram_results.csv'\ndf.to_csv(output_path, mode='a', header=not os.path.exists(output_path), index=False)","metadata":{"cell_id":"3049eabd14364e0486381b887c1b4917","source_hash":"14156602","execution_start":1686495444512,"execution_millis":16,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Pruning","metadata":{"tags":[],"cell_id":"4966faf1a50b49d28491c8881b7ed981","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"code","source":"from time import time\n\nmodel_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\nMODEL_NAME = 'model_1600hz2'\n\nsaved_model_dir = f'./{MODEL_NAME}'\nif not os.path.exists(saved_model_dir):\n    os.makedirs(saved_model_dir)\nmodel_for_export.save(saved_model_dir)","metadata":{"tags":[],"cell_id":"c73c30cf653b4630a6fb6784b991ffac","source_hash":"48791359","execution_start":1686495444531,"execution_millis":1825,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: ./model_1600hz2/assets\nINFO:tensorflow:Assets written to: ./model_1600hz2/assets\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Compression","metadata":{"tags":[],"cell_id":"0f87e32d4f884357a826f0477d49d9c1","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"}},{"cell_type":"code","source":"converter = tf.lite.TFLiteConverter.from_saved_model(f'./{MODEL_NAME}')\n\ntflite_model = converter.convert()","metadata":{"tags":[],"cell_id":"23dc103b613e48d6805521b96f58ce14","source_hash":"aae793ca","execution_start":1686495446360,"execution_millis":855,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2023-06-11 14:57:26.933110: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n2023-06-11 14:57:26.933158: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n2023-06-11 14:57:26.933868: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./model_1600hz2\n2023-06-11 14:57:26.937645: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n2023-06-11 14:57:26.937676: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./model_1600hz2\n2023-06-11 14:57:26.945378: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n2023-06-11 14:57:26.947110: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n2023-06-11 14:57:26.987413: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: ./model_1600hz2\n2023-06-11 14:57:27.011727: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 77863 microseconds.\n2023-06-11 14:57:27.063996: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"tflite_models_dir = './models/tflite_models'\nif not os.path.exists(tflite_models_dir):\n    os.makedirs(tflite_models_dir)","metadata":{"tags":[],"cell_id":"0516aa8b76a546fd9cde0d201096f595","source_hash":"fdfc1766","execution_start":1686495447219,"execution_millis":21,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":17},{"cell_type":"code","source":"tflite_model_name = os.path.join(tflite_models_dir, f'{MODEL_NAME}.tflite')\ntflite_model_name","metadata":{"tags":[],"cell_id":"73a9a3adb50f45f39c9bc77fc5eeba88","source_hash":"9658f457","execution_start":1686495447242,"execution_millis":5,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"'./models/tflite_models/model_1600hz2.tflite'"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"with open(tflite_model_name, 'wb') as fp:\n    fp.write(tflite_model)","metadata":{"tags":[],"cell_id":"9df601dc88d7462daf9dd7f7ec130759","source_hash":"90231985","execution_start":1686495447260,"execution_millis":45,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Zipping","metadata":{"tags":[],"cell_id":"acc3b9560da043449de4a682f16bb8e0","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"code","source":"import zipfile\n\nwith zipfile.ZipFile(f'{tflite_model_name}.zip', 'w', compression=zipfile.ZIP_DEFLATED) as f:\n    f.write(tflite_model_name, f'{MODEL_NAME}.tflite')","metadata":{"tags":[],"cell_id":"09acf0287a064e929f91de7d0c3b5414","source_hash":"5feb02d5","execution_start":1686495447306,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":20},{"cell_type":"code","source":"tflite_size = os.path.getsize(tflite_model_name) / 1024.0\nzipped_size = os.path.getsize(f'{tflite_model_name}.zip') / 1024.0\n\nprint(f'Original tflite size (pruned model): {tflite_size:.3f} KB')\nprint(f'Zipped tflite size (pruned model): {zipped_size:.3f} KB')","metadata":{"tags":[],"cell_id":"3d4b1287611c4c5b99b21af11d2ae513","source_hash":"db7b23f","execution_start":1686495447307,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Original tflite size (pruned model): 31.961 KB\nZipped tflite size (pruned model): 15.159 KB\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=05dea53b-e624-4a45-a48c-4f8a58ba823b' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"39389c23b9f446e1a085dab12aae4c8d","deepnote_execution_queue":[],"deepnote_persisted_session":{"createdAt":"2023-06-11T15:15:11.622Z"}}}